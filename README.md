# Introduction/Background
The US stock market is one of the most important financial institutions in the world. Despite being unpredictable, it has allowed a significant number of Americans to build retirement funds and accumulate long-term wealth. Experienced traders tend to recommend different “rules of thumb” and methods of logical reasoning for deciding on market investments, but our team asks the following question: can supervised and unsupervised machine learning techniques be applied to the financial data of companies to help make better predictions regarding different stocks’ performances?

# Problem Definition
Our task for this problem is to predict whether a stock’s value will increase (positive label) or decrease (negative label) by the end of a given year and thereby label the stock as either a desirable (positive label) or an undesirable (negative label) stock to purchase at the start of that year. The data that will be used to help solve this problem is a dataset of over 200 financial indicators for almost 4000 companies with stocks on the US stock market for five years (2014 to 2018) [1]. These indicators include everything from revenue to net debt as a percentage of assets. The data also contains the end-of-year price change for the stock. There are several performance measures that can be used to measure the resulting model’s effectiveness. This team uses accuracy, precision, and root mean squared error (RMSE) to evaluate the performance on the testing dataset. Goals include having an accurate model (correctly predicting stocks that are desirable to purchase and that are undesirable to purchase among all the stocks that could be purchased in a year) and a precise/consistent model (avoiding false predictions that a stock is desirable, which could lead to unwanted purchases). The RMSE is used to evaluate whether the model is converging, underfitting, or overfitting and how well the model is performing on both testing and training datasets.

# Data Collection
The raw financial data has been sourced from Kaggle datasets which have itself been sourced from the Financial Modeling Prep API [1]. The data itself is split into five CSV format files with each file representing a financial year (with over 200 financial indicators) for almost 4000 companies with stocks on the US stock market between the years of 2014 and 2018. The team imported these CSV format files into a Python environment as Pandas data frames, relabelled them, removed incomplete records, and verified that the data types for all features are comparable (using relational operators/methods).

# Methods
Our methodology for building a model that can predict stocks’ performances can be broken down into data cleaning/feature reduction and training/testing the prediction model itself. First, the team explored using Principal Component Analysis (PCA) (a linear dimension-reduction technique) to decrease the dimensionality of the model by finding new axes that maximize the variance (and information) preserved in the data [8]. From here, the team segmented and inspected the data for outliers. Since using PCA resulted in lower than desired accuracies (60.372% at best), the team then used “Missing Value Ratio” feature selection, which selects features with a fewer number of “NaN” (missing) values and handles these missing values [9]. This team simply removed these missing values, but in the future, it is possible that replacing these missing values with a mean or median of the data could help improve accuracy. After the data's dimensionality was reduced and cleaned, the team moved forward with optimizing hyperparameters for the prediction models.

For the prediction models, the team first used SVM. SVM is a popular machine learning technique for stock performance prediction, and with our high dimensions in data, the team considered SVM to be a good potential solution [2], [3], [4]. The team also used neural networks since they are typically able to deal with a high number of features and fit a precise model. However, with data with many features like this one, overfitting is a serious problem. Therefore, the team applied a random forest model since these models have well-controlled methods for mitigating the risk of overfitting training data. For example, pruning, limiting the depth of the decision trees, and ensemble learning itself can help prevent overfitting. 

# Results and Discussion
We used our data from 2014 - 2018 to train our model, and we divided our data into training and testing sets. We determine how well the model predicts a binary label that describes whether or not the price of a stock increased over a given year.

For our SVM model, we focus on analyzing the training and testing error, the accuracy of the model for different numbers of features (via cross-validation with 5 folds), and the precision of the model for different numbers of features (via cross-validation with 5 folds).

## Data Cleaning
First, the team needed to clean the data so that irrelevant and incorrect data would not negatively impact the model. We achieved this by using the Pandas library to read all the CSV files that contained the stock data from 2014 to 2018 into five separate data frames. We then noticed that the data samples were denoted by the stocks’ ticker symbols, so the team renamed the ticker columns from “Unnamed: 0” to “Ticker.” Afterward, the team used Python’s set function to construct five sets – one for each data frame. This allowed the team to get rid of duplicate entries. We then found the intersection of these five sets, which limits the data to only having the data for companies that are present in all five years. Finally, the team filtered the data frames based on the intersections so that the model can be more accurate.

## Data Pre-Processing (Feature Selection)
Before fitting the model, we needed to first normalize the data. This is done by using scikit-learn’s StandardScaler class in the preprocessing package. First, the standard scalar is fit to the training data. This creates a function that normalizes the training data to have a mean of zero and a variance of one by centering and scaling along the features’ axis. The transform method is then used on the training and testing sets to normalize them to these parameters. Essentially, this computes a standard score of a sample, x, by calculating z = (x - u) / s, where u is the mean of the training samples, and s is the standard deviation of the training samples. This standardization is a common requirement of machine learning estimators because the estimators may behave badly if individual features do not “look like” standard normally distributed data (0 mean and unit variance); many elements in the objective function of a learning algorithm, such as the RBF kernel of Support Vector Machines (SVMs), assume that features are centered around 0 and have a variance of the same order [7].

The team next tried to use Principal Component Analysis (PCA) for dimensionality reduction and feature selection. We implemented this with scikit-learn’s sklearn.decomposition.PCA class, which uses Singular Value Decomposition (SVD) to perform linear dimensionality reduction on the data to project it to a lower-dimensional space. The team can construct an instance of this class with a different number of components (n_components) to keep a different amount of the original variance (information) from the original dataset. We use this class’s fit() function on the training data, and we use the transform() function on both the training and the testing data. The model is fitted with the training data, and the dimensionality reduction is applied to both the training and the testing data. With transform, both the training and testing data are projected onto the first principal components that were extracted from a training set [8]. However, after experimenting and tuning different hyperparameters (like the number of first principal components to extract with PCA, the testing-training data split, the type of prediction method, and the solver used with that prediction method), the best accuracy that the team achieved was 60.372%. This accuracy was achieved when extracting 13 first principal components with PCA, using an MLP classifier (classical type of neural network, a multilayer perceptron), using the “sgd” (stochastic gradient descent) solver, and using an 85% training and 15% testing split. PCA is a linear dimension-reduction technique that might not be able to effectively find a single direction of maximum variance for the principal component without sacrificing some accuracy; the data might not appear linearly separable.

To improve the accuracy and to simultaneously address the larger number of missing values (“NaN”) values in the dataset, the team instead implemented the “Missing Value Ratio” (MVR) method of feature selection (and dimensionality reduction). With this method, we calculate the number of missing values in each column (for each feature) and divide it by the total number of observations (samples) in order to get the ratio of missing values for each feature. We can then sort these features from the lowest to the greatest missing value ratios and leave out the features that have the greatest missing value ratios. We can either set a threshold to drop all features that have a missing value ratio that is greater than the threshold, or we can simply keep a certain number of features with the least missing value ratios. We tried the latter. We then remove the rows (samples) that contain NaN values. To further improve performance, we could potentially replace the NaN values with statistical measures like mean, median, or mode [9]. Data is standardized again before passing it into a prediction model. To optimize these hyperparameters, such as the number of features to select with the “Missing Value Ratio” method, we use cross-validation with 5 folds. Since we discovered that the data for 2017 had a very large proportion of NaN values, we decided to only use the data for 2014, 2015, and 2018. 
	With a large amount of the NaN values removed, the data set could be further explored. For further exploration, we first used the Gini importance values to further cut down our number of features. To do this, we introduce a new hyperparameter that is the minimum importance required to keep a feature.

![image GiniImportanceOne](https://github.gatech.edu/raw/wminix3/CS4641-Stocks/main/Gini1.PNG?token=AAAIXMUDGKLQBDLFZ72QM5DBXF6DQ)

Through cross validation techniques, the best value for the minimum Gini importance was deemed to be 0.065. Removing the features with importances below the cut off will leave the model with the features shown in the graph below.

![image GiniImportanceTwo](https://github.gatech.edu/raw/wminix3/CS4641-Stocks/main/Gini2.PNG?token=AAAIXMXEMTXFWXEIFLLKBMDBXF6T2)

As shown, the most important feature is still the price to cash flow ratio. This is a ratio of the stock price at the beginning of the year to the cash flow per share of the stock. With this method of feature selection and preprocessing, we get an accuracy of up to 80% and a precision of ~80% for the SVM model and an accuracy of up to 78% and a precision of ~80% for the Random forest model.

## SVM Results and Metrics
To begin evaluating the model, each feature was added one at a time, and the model was trained and tested using five-fold cross-validation. The following results describe the SVM model before the Gini importance values were used for further feature reduction. It is shown in the figure below that as we increase the number of features, the error of the model tends to decrease. It appears that when features 10 through 12 are added, our model has the biggest decrease in error. With more than twelve features, the error begins to flatten, and our model seems to begin to converge to a good (more accurate and more generalized) solution.

![image TestTrain](TestTrainErrorSVM.png?token=AAAIXMTOC4QQ6R73CNI357LBTXLCW)

![image Precision](SVM%20Percision%20Model.png?token=AAAIXMUUX6XIAW2CB2XFRMDBTXLGA)

The graph above shows the precision of the SVM model as a classifier for stock increase. Precision was deemed to be a highly valued metric because it gives insight on how accurate our model is when classifying a point as positive, or in this case, it is a metric that describes how well our model predicts stocks that increase in value (with a lower occurrence of predicting that stocks will increase in value when they do not). Of the stocks that are predicted to increase in value, it is desired that most of these stocks will actually increase in value to prevent an undesirable loss of money. For example, you don’t want to be told that a stock will increase in value, buy it, and have it decrease in value. The value of precision increases with the number of features until it peaks with twelve features with a precision of ~80%. From here the precision decreases and stalls. The most likely cause for this decrease in precision is based on the relative number of data points. As the number of features increases, the relative number of data points to features decreases. This is especially true for the last few features as they were deemed to be the features that contribute the least to our model compared to how many data points were removed to add them. These are the features with the greatest number of missing values. Just like in the error plot, features 10-12, or the cash flow-to-debt ratio, total debt to capitalization, and earnings yield features seem to increase the precision the most.

![image Accuracy](SVM%20Accuracy%20Model.png?token=AAAIXMWPBQIEDWESWX6YLITBTXLHU)

Accuracy was also used to measure the models’ performance. Accuracy is one of the more basic and straightforward metrics that describes how many labels the model predicts correctly out of the total number of predictions. The accuracy was calculated using the accuracy_score function from the sklearn metrics package. The accuracy appears to be identical to the precision graph. This might suggest that our model’s errors are more biased towards false positives. The accuracy increases with the number of features until it peaks with 12 features at an accuracy of ~80%.

## Neural Network Results and Metrics
While a support vector machine model was producing acceptable results, the team wanted to see how other models would perform. With PCA for feature selection, our neural network model achieved a 60.372% accuracy. With data with many features like this one, overfitting appears to be a serious problem. With using MVR for feature selection, our neural network model achieved a somewhat poor accuracy (not much better than randomly guessing), so we quickly steered clear of neural networks in favor of the better-performing SVM.

## Random Forest Results and Metrics
We also decided to branch out into ensemble learning. We started first with a decision tree, but it became immediately clear that the tree was overfitting. Even with a pruning cross validation, the model was still performing close to random guessing. This led the team to move towards assembling a group of weak learners with a random forest. At first, the random forest model’s performance was better than random guessing, but with an average accuracy of ~60%, it was not comparable to the results of the support vector machine model. We were unsatisfied with these results and decided to take a step back to reevaluate our process. This is when we started looking into Gini importance values. By revamping our feature selection to utilize a combination of the Gini importance and the missing value ratio of each feature, the random forest model had an accuracy of up to 79% using K-fold cross validation and an average accuracy of 73%. Unlike its’ support vector machine counterpart, it was found that the random forest was able to keep its high accuracy when predicting the stock prices from years that it was not trained on! (No sample from the testing set came from the same year as a sample in the training set). In other words, the random forest model is able to meet the project's goal of predicting stock price performance for arbitrary years.

## The Chosen Model: A Decision Tree with an SVM
What would happen if we used the updated feature selection with an SVM model? By first training a decision tree to get the Gini importances, the new feature selection method was used with the support vector machine. Using K-fold cross validation for hyperparameter optimization and the new feature selection method in addition to Missing Value Ratio (MVR), the new SVM model achieved a max accuracy of 82% and an accuracy of 67% on a financial year that it was not trained on! (No sample from the testing set came from the same year as a sample in the training set). We then compared this model's accuracy against the other models, and the results are shown in the table below.

![image ResultsTable](https://github.gatech.edu/raw/wminix3/CS4641-Stocks/main/TableResults.PNG?token=AAAIXMXN6ELMLNDHL5VSNQ3BXF7IO)

# Conclusions
At first glance, it is difficult to tell which model is optimal, but due to its high accuracy and reliability, the team has decided to choose the SVM model with both Gini importance and Missing Value Ratio (for feature selection) as the final model.

The problem of stock market prediction has captured the attention of plenty of Americans who wish to prepare for retirement and accumulate wealth. Our team found this problem to be both a challenging and an informative experience for experimenting with and comparing several machine learning techniques. The power of both SVM and different methods of feature selection (and combining Gini importances and Missing Value Ratio) has been proven through their overall successful application to the challenge of stock performance prediction. Ensemble learning (with random forests) also appears promising. Ideally, our team would have liked to have achieved a consistent 80-90% accuracy, but the challenge of stock prediction is a challenge that people have approached for generations without a definite solution. This problem poses an interesting question regarding how well 200 financial indicators can capture all the events that occur during a year that can impact a stock’s performance. For example, can our data and our model capture the impact of large events, such as presidential elections, recessions, or pandemics? 

In the future, our team would like to explore utilizing means, medians, and modes for replacing missing values instead of simply removing them so that more samples can be used for training. The team would also like to experiment with other stock performance datasets to see how well results generalize and if more/different data produce better results, and the team would like to return to ensemble learning, SVM, and different combinations of machine learning techniques and prediction models. Additionally, we believe that clean, real-time data can help with achieving really good results since obtaining batches of data can result in data that is very difficult to clean and results in lower accuracy. For now, we are satisfied with our promising results that beat randomly guessing.

# References
[1] N. Carbone, “200+ Financial Indicators of US Stocks (2014-2018),” Kaggle, 2019. [Online]. Available: https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018. [Accessed: October 6, 2021]

[2] S. Foo, N. Powell, and M. Weatherspoon, “Supervised and Unsupervised Methods for Stock Trend Forecasting,” in 2008 40th Southeastern Symposium on System Theory (SSST). IEEE, [Online document], 2008. Available: IEEE Xplore, https://ieeexplore.ieee.org/abstract/document/4480220. [Accessed: October 6, 2021]

[3] Scikit-learn Developers, “Support Vector Machines,” Scikit Learn. [Online]. Available: https://scikit-learn.org/stable/modules/svm.html. [Accessed: October 6, 2021]

[4] Y. Chen and Y. Hao, “A feature weighted support vector machine and K-nearest neighbor algorithm for stock market indices prediction,” Expert Systems with Applications, vol. 80, p. 340-355, September 2017. [Online serial]. Available: ScienceDirect, https://www.sciencedirect.com/science/article/pii/S0957417417301367. [Accessed: October 6, 2021]

[5] R. Singh and S. Srivastava, “Stock prediction using deep learning,” Multimedia Tools and Applications, vol. 76, p. 18569-18584, 2017. [Online]. Available: https://link.springer.com/article/10.1007/s11042-016-4159-7. [Accessed: October 6, 2021]

[6] B. Malkiel, “‘Buy and Hold’ Is Still a Winner”, The Wall Street Journal, 2010. [Online]. Available: https://www.wsj.com/articles/SB10001424052748703848204575608623469465624. [Accessed: October 6, 2021].

[7] “sklearn.preprocessing.StandardScaler,” scikit-learn, 2021. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html. [Accessed: November 16, 2021].

[8] “sklearn.decomposition.PCA,” scikit-learn, 2021. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit. [Accessed: November 16, 2021].

[9] H. Singh, “Beginner’s Guide to Missing Value Ratio and its Implementation,” 2021. [Online]. Available: https://www.analyticsvidhya.com/blog/2021/04/beginners-guide-to-missing-value-ratio-and-its-implementation/. [Accessed: November 16, 2021].
