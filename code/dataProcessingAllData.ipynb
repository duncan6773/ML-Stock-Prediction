{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c4c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16084ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all \n",
    "data_2014 = pd.read_csv(\"./2014_Financial_Data.csv\")\n",
    "data_2015 = pd.read_csv(\"./2015_Financial_Data.csv\")\n",
    "data_2016 = pd.read_csv(\"./2016_Financial_Data.csv\")\n",
    "data_2017 = pd.read_csv(\"./2017_Financial_Data.csv\")\n",
    "data_2018 = pd.read_csv(\"./2018_Financial_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab159103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the companies are denoted by their stock ticker but the ticker column is unnamed\n",
    "#this cell renames that to \"Ticker\" for all dataframes\n",
    "data_2014.rename( columns={'Unnamed: 0':'Ticker'}, inplace=True )\n",
    "data_2015.rename( columns={'Unnamed: 0':'Ticker'}, inplace=True )\n",
    "data_2016.rename( columns={'Unnamed: 0':'Ticker'}, inplace=True )\n",
    "data_2017.rename( columns={'Unnamed: 0':'Ticker'}, inplace=True )\n",
    "data_2018.rename( columns={'Unnamed: 0':'Ticker'}, inplace=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa80925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct 5 different sets of ticker labels, 1 for eachh data frame\n",
    "ticker_set_2014 = set(data_2014[\"Ticker\"])\n",
    "ticker_set_2015 = set(data_2015[\"Ticker\"])\n",
    "ticker_set_2016 = set(data_2016[\"Ticker\"])\n",
    "ticker_set_2017 = set(data_2017[\"Ticker\"])\n",
    "ticker_set_2018 = set(data_2018[\"Ticker\"])\n",
    "#find the intersection of the sets. This limits us to those companies for whom we have data from all five years\n",
    "all_common_companies = list(set.intersection(ticker_set_2014, ticker_set_2015, ticker_set_2016, ticker_set_2017,\n",
    "                                        ticker_set_2018))\n",
    "#filter dataframes using the list of common companies we created above\n",
    "data_2014 = data_2014[data_2014[\"Ticker\"].isin(all_common_companies)]\n",
    "data_2015 = data_2015[data_2015[\"Ticker\"].isin(all_common_companies)]\n",
    "data_2016 = data_2016[data_2016[\"Ticker\"].isin(all_common_companies)]\n",
    "data_2017 = data_2017[data_2017[\"Ticker\"].isin(all_common_companies)]\n",
    "data_2018 = data_2018[data_2018[\"Ticker\"].isin(all_common_companies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be7cc2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Alright, lets try to get out the data we want from the 2014 array and make a new Pandas data frame with it\n",
    "\n",
    "# Removed \"Ticker\", from the first entry of each of these\n",
    "new_2014 = data_2014[['EPS','EBIT Margin', 'Net Profit Margin','Net Cash/Marketcap','priceBookValueRatio','priceEarningsRatio', 'priceCashFlowRatio',\n",
    " 'returnOnAssets','inventoryTurnover','cashFlowToDebtRatio','totalDebtToCapitalization','Earnings Yield','Debt to Assets', 'Current ratio','Graham Net-Net']]\n",
    "labels_2014 = data_2014[['Class']]\n",
    "new_2015 = data_2015[['EPS','EBIT Margin', 'Net Profit Margin','Net Cash/Marketcap','priceBookValueRatio','priceEarningsRatio', 'priceCashFlowRatio',\n",
    " 'returnOnAssets','inventoryTurnover','cashFlowToDebtRatio','totalDebtToCapitalization','Earnings Yield','Debt to Assets', 'Current ratio','Graham Net-Net']]\n",
    "labels_2015 = data_2015[['Class']]\n",
    "new_2016 = data_2016[['EPS','EBIT Margin', 'Net Profit Margin','Net Cash/Marketcap','priceBookValueRatio','priceEarningsRatio', 'priceCashFlowRatio',\n",
    " 'returnOnAssets','inventoryTurnover','cashFlowToDebtRatio','totalDebtToCapitalization','Earnings Yield','Debt to Assets', 'Current ratio','Graham Net-Net']]\n",
    "labels_2016 = data_2016[['Class']]\n",
    "new_2017 = data_2017[['EPS','EBIT Margin', 'Net Profit Margin','Net Cash/Marketcap','priceBookValueRatio','priceEarningsRatio', 'priceCashFlowRatio',\n",
    " 'returnOnAssets','inventoryTurnover','cashFlowToDebtRatio','totalDebtToCapitalization','Earnings Yield','Debt to Assets', 'Current ratio','Graham Net-Net']]\n",
    "labels_2017 = data_2017[['Class']]\n",
    "new_2018 = data_2018[['EPS','EBIT Margin', 'Net Profit Margin','Net Cash/Marketcap','priceBookValueRatio','priceEarningsRatio', 'priceCashFlowRatio',\n",
    " 'returnOnAssets','inventoryTurnover','cashFlowToDebtRatio','totalDebtToCapitalization','Earnings Yield','Debt to Assets', 'Current ratio','Graham Net-Net']]\n",
    "labels_2018 = data_2018[['Class']] # CHANGED THIS TO 2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b1d4a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the max number of NaN to data points\n",
    "nanRatio = 0.1\n",
    "\n",
    "\n",
    "#Now I am going to try to itter over the rows and figure out how many NaNs we have \n",
    "dataPoints2014 = new_2014.shape[0]\n",
    "dataPoints2015 = new_2015.shape[0]\n",
    "dataPoints2016 = new_2016.shape[0]\n",
    "dataPoints2017 = new_2017.shape[0]\n",
    "dataPoints2018 = new_2018.shape[0]\n",
    "#Now I am going to try to itter over the rows and figure out how many NaNs we have \n",
    "dataPoints2014 = new_2014.shape[0]\n",
    "dataPoints2015 = new_2015.shape[0]\n",
    "dataPoints2016 = new_2016.shape[0]\n",
    "dataPoints2017 = new_2017.shape[0]\n",
    "dataPoints2018 = new_2018.shape[0]\n",
    "\n",
    "null_2014 = new_2014.isnull().sum(axis=0)\n",
    "null_2015 = new_2015.isnull().sum(axis=0)\n",
    "null_2016 = new_2016.isnull().sum(axis=0)\n",
    "null_2017 = new_2017.isnull().sum(axis=0)\n",
    "null_2018 = new_2018.isnull().sum(axis=0)\n",
    "#My Plan is to delete rows with NaN but If the majority rows have NaN then \n",
    "#it would be better to delete the column \n",
    "#making a ratio of NaN to data Points to mask with\n",
    "null_2014 = null_2014 / dataPoints2014\n",
    "null_2015 = null_2015 / dataPoints2015\n",
    "null_2016 = null_2016 / dataPoints2016\n",
    "null_2017 = null_2017 / dataPoints2017\n",
    "null_2018 = null_2018 / dataPoints2018\n",
    "#Things with high Null Ratio\n",
    "#Current Ratio 0.25\n",
    "#cashFlowToDebtRatio 0.41\n",
    "#return on assets 0.25\n",
    "#Price Cash Flor 0.25\n",
    "#Price Booking 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdf04dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brice\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\brice\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\brice\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\brice\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy 0.5948581560283688\n",
      "SVM Accuracy 0.5948581560283688\n",
      "NN Accuracy 0.6019503546099291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(len-1):\\n    #Getting out how many labels we want \\n    myFeat = colLabel[1:i+2]\\n    myDataSet = totalData[myFeat]\\n    #Now need to remove the rows with NaN \\n    myMask = myDataSet.isnull().sum(axis = 1)\\n    myMask = myMask == 0\\n    myDataSet = myDataSet[myMask]\\n    myLabels = totalLabels[myMask]\\n    foldErrorSVM = 0\\n    foldErrorNN = 0\\n    #Gonna split the data into K folds and train an svm model \\n    for trainIdx,testIdx in mySplitter.split(myDataSet):\\n        #Note convert your pandas data frame into a numpy array \\n        #\\n        trainData = myDataSet.iloc[trainIdx]\\n        trainData = trainData.to_numpy()\\n        \\n    #\\n        testData = myDataSet.iloc[testIdx]\\n        testData = testData.to_numpy()\\n        trainLabel = myLabels.iloc[trainIdx]\\n        trainLabel = trainLabel.to_numpy()[:,0]\\n        testLabel = myLabels.iloc[testIdx]\\n        testLabel = testLabel.to_numpy()[:,0]\\n        \\n        #Need to standardize data set \\n        #trainPoints = trainData.shape[0]\\n        #trainMean = np.sum(trainData,axis = 0) / trainPoints\\n        #trainData = trainData - trainMean\\n    \\n        scalor.fit(trainData)\\n        trainData = scalor.transform(trainData)\\n        testData = scalor.transform(testData)\\n        \\n        # Do PCA before using learning methods for more feature selection.\\n        #pca = PCA(0.95)\\n        #pca.fit(trainData)\\n        #trainData = pca.transform(trainData)\\n        #testData = pca.transform(testData)\\n\\n        #Fitting an SVM and Neural Network to the training data set \\n        myModel = mySvm.fit(trainData,trainLabel)\\n        clf.fit(trainData,trainLabel)\\n        #Predicting the Test Data \\n        myTestSVM = mySvm.predict(testData)\\n        myTestNN = clf.predict(testData)\\n        #Using Euclidean distance to get the fold error\\n        myErrorSVM = myTestSVM == testLabel\\n        myErrorNN = myTestNN == testLabel\\n        myErrorSVM = np.sum(myErrorSVM)\\n        myErrorNN = np.sum(myErrorNN)\\n        #note we need to divide by the number of data points each time because each loop \\n        #the number of data points is getting smaller \\n        foldErrorSVM = foldErrorSVM + (myErrorSVM/ trainData.shape[0])\\n        foldErrorNN = foldErrorNN + (myErrorNN/ trainData.shape[0])\\n    totErrorSVM[i]= foldErrorSVM\\n    totErrorNN[i] = foldErrorNN\\n\\n#Lets plot how the error changes\\nplt.plot(range(len-1),totErrorSVM,'*')\\nplt.plot(range(len-1),totErrorNN,'g*')\\nplt.xlabel('Number of Features')\\nplt.ylabel('accuracy ')\\nplt.legend(['SVM', 'Neural Network'])\\nplt.show()\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gonna get out the feature with the least Nan ratio\n",
    "nullItter = null_2014.sort_values(axis = 0)\n",
    "#Now need to get the column label\n",
    "colLabel = nullItter.index\n",
    "len = colLabel.size\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA # Added this\n",
    "from sklearn.model_selection import train_test_split # Added this import for splitting training and testing data\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scalor = StandardScaler()\n",
    "#Gonna start with the column with the least nans and for each itteration add\n",
    "#another feature\n",
    "totErrorSVM = np.zeros([len-1]) \n",
    "totErrorNN = np.zeros([len-1]) \n",
    "#mySvm = svm.SVC() # kernel = rbf. rbf: 0.589095744680851.\n",
    "#mySvm = svm.SVC(kernel='linear') # kernel = linear. linear: 0.5897606382978723\n",
    "mySvm = svm.SVC(kernel='poly') # kernel = poly. poly: 0.5948581560283688\n",
    "#mySvm = svm.SVC(kernel='sigmoid') # kernel = sigmoid. sigmoid: 0.5518617021276596\n",
    "#mySvm = svm.SVC(kernel='precomputed') # kernel = precomputed. precomputed: error\n",
    "#mySvm = svm.SVC(kernel='rbf') # kernel = rbf. rbf: 0.589095744680851\n",
    "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,40), random_state=1) # lbfgs: 0.5806737588652482\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5,hidden_layer_sizes=(100,40), random_state=1) # sgd: 0.5939716312056738\n",
    "#clf = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(100,40), random_state=1) # adam: 0.5842198581560284\n",
    "logisticRegr = LogisticRegression(solver='lbfgs') # 0.5966312056737588 accuracy\n",
    "#logisticRegr = LogisticRegression(solver='newton-cg') # 0.5966312056737588 accuracy\n",
    "#logisticRegr = LogisticRegression(solver='liblinear') # 0.5957446808510638\n",
    "#logisticRegr = LogisticRegression(solver='sag') # 0.5948581560283688\n",
    "#logisticRegr = LogisticRegression(solver='saga') # 0.5948581560283688\n",
    "mySplitter = KFold(n_splits=5)\n",
    "#for now, I am going to combine all data sets to make a super data set\n",
    "totalData = new_2014.append(new_2015,sort=False)\n",
    "totalData = totalData.append(new_2016,sort=False)\n",
    "totalData = totalData.append(new_2017,sort=False)\n",
    "totalData = totalData.append(new_2018,sort=False) # I CHANGED THIS FROM 2017 to 2018!!!\n",
    "totalLabels = labels_2014.append(labels_2015,sort=False)\n",
    "totalLabels = totalLabels.append(labels_2016,sort=False)\n",
    "totalLabels = totalLabels.append(labels_2017,sort=False)\n",
    "totalLabels = totalLabels.append(labels_2018,sort=False)\n",
    "\n",
    "# Added this outside the loop to try to feature select only with PCA\n",
    "myMask = totalData.isnull().sum(axis = 1)\n",
    "myMask = myMask == 0\n",
    "myDataSet = totalData[myMask]\n",
    "myLabels = totalLabels[myMask]\n",
    "\n",
    "#trainData, testData, trainLabel, testLabel = train_test_split(myDataSet, myLabels, test_size=0.2, random_state=0) # Test 20%: SVM: 0.5904255319148937 accuracy NN: 0.5704787234042553\n",
    "#trainData, testData, trainLabel, testLabel = train_test_split(myDataSet, myLabels, test_size=0.3, random_state=0) # Test 30%: SVM: 0.5855496453900709 NN: 0.563386524822695\n",
    "#trainData, testData, trainLabel, testLabel = train_test_split(myDataSet, myLabels, test_size=0.1, random_state=0) # Test 10%: SVM: 0.589095744680851 NN: 0.5731382978723404\n",
    "trainData, testData, trainLabel, testLabel = train_test_split(myDataSet, myLabels, test_size=0.15, random_state=0) # Test 15%: SVM: 0.5948581560283688 NN: 0.5806737588652482 \n",
    "#trainData, testData, trainLabel, testLabel = train_test_split(myDataSet, myLabels, test_size=0.25, random_state=0) # Test 25%: SVM: 0.5867021276595744 NN: 0.575\n",
    "#trainData, testData, trainLabel, testLabel = train_test_split(myDataSet, myLabels, test_size=0.12, random_state=0) # Test 12%: SVM: 0.592469545957918 NN: 0.5736434108527132\n",
    "#trainData, testData, trainLabel, testLabel = train_test_split(myDataSet, myLabels, test_size=0.16, random_state=0) # Test 13%: SVM: 0.5930470347648262 NN: 0.5705521472392638\n",
    "# Test 14%: SVM: 0.5944919278252612 NN: 0.5650522317188984\n",
    "# Test 16%: SVM: 0.5943474646716542 NN: 0.5810473815461347\n",
    "#print(trainData)\n",
    "#scalor.fit(trainData, y=trainLabel) # BETTER FOR NN\n",
    "scalor.fit(trainData) # BETTER FOR SVM\n",
    "trainData = scalor.transform(trainData) # BETTER FOR SVM\n",
    "testData = scalor.transform(testData) # BETTER FOR SVM\n",
    "#trainData = scalor.fit_transform(trainData, y=trainLabel) # BETTER FOR NN (Basically switch places with accuracy)\n",
    "#testData = scalor.fit_transform(testData, y=testLabel) # BETTER FOR NN\n",
    "\n",
    "# Do PCA before using learning methods for more feature selection.\n",
    "#pca = PCA(0.95) # SVM: 0.5948581560283688 NN: 0.5939716312056738 log reg: 0.5957446808510638\n",
    "#pca = PCA(0.8) # SVM: 0.5939716312056738 NN: 0.598404255319149 log reg: 0.5966312056737588\n",
    "#pca = PCA(0.99) # SVM: 0.5948581560283688 NN: 0.6019503546099291 log reg: 0.5966312056737588\n",
    "pca = PCA(n_components = 13) # 13: 0.5948581560283688, 14: 0.5939716312056738, 12: 0.5948581560283688, 11: 0.5939716312056738, 10: 0.5930851063829787, 9: 0.5939716312056738, 8: 0.5948581560283688, 7: 0.5913120567375887, 6: 0.5930851063829787, 5: 0.5930851063829787, 4: 0.5921985815602837, 3: 0.5913120567375887, 2: 0.5913120567375887, 1: 0.5913120567375887\n",
    "#pca = PCA(0.999) # Same as above\n",
    "# No PCA: SVM: 0.5939716312056738 NN: 0.5975177304964538 log reg: 0.5948581560283688\n",
    "pca.fit(trainData)\n",
    "trainData = pca.transform(trainData)\n",
    "testData = pca.transform(testData)\n",
    "\n",
    "#Fitting an SVM and Neural Network to the training data set \n",
    "myModel = mySvm.fit(trainData,trainLabel)\n",
    "clf.fit(trainData,trainLabel)\n",
    "logisticRegr.fit(trainData, trainLabel)\n",
    "#Predicting the Test Data \n",
    "myTestSVM_predict = mySvm.predict(testData)\n",
    "myTestNN_predict = clf.predict(testData)\n",
    "logisticRegr_predict = logisticRegr.predict(testData)\n",
    "print(\"Logistic Regression Accuracy\", logisticRegr.score(testData, testLabel))\n",
    "#print(metrics.accuracy_score(testLabel, logisticRegr_predict))\n",
    "print(\"SVM Accuracy\", metrics.accuracy_score(testLabel, myTestSVM_predict))\n",
    "print(\"NN Accuracy\", metrics.accuracy_score(testLabel, myTestNN_predict))\n",
    "\"\"\"\n",
    "for i in range(len-1):\n",
    "    #Getting out how many labels we want \n",
    "    myFeat = colLabel[1:i+2]\n",
    "    myDataSet = totalData[myFeat]\n",
    "    #Now need to remove the rows with NaN \n",
    "    myMask = myDataSet.isnull().sum(axis = 1)\n",
    "    myMask = myMask == 0\n",
    "    myDataSet = myDataSet[myMask]\n",
    "    myLabels = totalLabels[myMask]\n",
    "    foldErrorSVM = 0\n",
    "    foldErrorNN = 0\n",
    "    #Gonna split the data into K folds and train an svm model \n",
    "    for trainIdx,testIdx in mySplitter.split(myDataSet):\n",
    "        #Note convert your pandas data frame into a numpy array \n",
    "        #\n",
    "        trainData = myDataSet.iloc[trainIdx]\n",
    "        trainData = trainData.to_numpy()\n",
    "        \n",
    "    #\n",
    "        testData = myDataSet.iloc[testIdx]\n",
    "        testData = testData.to_numpy()\n",
    "        trainLabel = myLabels.iloc[trainIdx]\n",
    "        trainLabel = trainLabel.to_numpy()[:,0]\n",
    "        testLabel = myLabels.iloc[testIdx]\n",
    "        testLabel = testLabel.to_numpy()[:,0]\n",
    "        \n",
    "        #Need to standardize data set \n",
    "        #trainPoints = trainData.shape[0]\n",
    "        #trainMean = np.sum(trainData,axis = 0) / trainPoints\n",
    "        #trainData = trainData - trainMean\n",
    "    \n",
    "        scalor.fit(trainData)\n",
    "        trainData = scalor.transform(trainData)\n",
    "        testData = scalor.transform(testData)\n",
    "        \n",
    "        # Do PCA before using learning methods for more feature selection.\n",
    "        #pca = PCA(0.95)\n",
    "        #pca.fit(trainData)\n",
    "        #trainData = pca.transform(trainData)\n",
    "        #testData = pca.transform(testData)\n",
    "\n",
    "        #Fitting an SVM and Neural Network to the training data set \n",
    "        myModel = mySvm.fit(trainData,trainLabel)\n",
    "        clf.fit(trainData,trainLabel)\n",
    "        #Predicting the Test Data \n",
    "        myTestSVM = mySvm.predict(testData)\n",
    "        myTestNN = clf.predict(testData)\n",
    "        #Using Euclidean distance to get the fold error\n",
    "        myErrorSVM = myTestSVM == testLabel\n",
    "        myErrorNN = myTestNN == testLabel\n",
    "        myErrorSVM = np.sum(myErrorSVM)\n",
    "        myErrorNN = np.sum(myErrorNN)\n",
    "        #note we need to divide by the number of data points each time because each loop \n",
    "        #the number of data points is getting smaller \n",
    "        foldErrorSVM = foldErrorSVM + (myErrorSVM/ trainData.shape[0])\n",
    "        foldErrorNN = foldErrorNN + (myErrorNN/ trainData.shape[0])\n",
    "    totErrorSVM[i]= foldErrorSVM\n",
    "    totErrorNN[i] = foldErrorNN\n",
    "\n",
    "#Lets plot how the error changes\n",
    "plt.plot(range(len-1),totErrorSVM,'*')\n",
    "plt.plot(range(len-1),totErrorNN,'g*')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('accuracy ')\n",
    "plt.legend(['SVM', 'Neural Network'])\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
